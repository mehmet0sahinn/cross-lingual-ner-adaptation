{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"This notebook explores a multilingual Named Entity Recognition (NER) pipeline using the XLM-RoBERTa model, with a focus on cross-lingual transfer learning.\n\nThe model is fine-tuned on English data (PAN-X from the XTREME benchmark) and evaluated on Turkish in both zero-shot and few-shot settings. By gradually increasing the number of training samples in the target language, we analyze how low-resource adaptation impacts NER performance.\n\nKey goals of this notebook include:\n* Evaluating cross-lingual NER transfer from English to Turkish,\n* Investigating few-shot adaptation behavior under varying data sizes,\n* Providing a reproducible baseline for multilingual NER fine-tuning using Hugging Face Transformers.","metadata":{}},{"cell_type":"markdown","source":"# 1 Load The Dataset","metadata":{}},{"cell_type":"code","source":"!pip install -q datasets transformers seqeval --no-deps\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:48:20.920925Z","iopub.execute_input":"2025-05-22T19:48:20.921220Z","iopub.status.idle":"2025-05-22T19:48:25.700538Z","shell.execute_reply.started":"2025-05-22T19:48:20.921196Z","shell.execute_reply":"2025-05-22T19:48:25.699798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\nfrom collections import defaultdict\n\ndef load_panx_datasets(langs):\n    panx_ch = defaultdict(DatasetDict)\n    for lang in langs:\n        ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n        for split in ds:\n            panx_ch[lang][split] = ds[split].shuffle(seed=0)\n    return panx_ch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:48:25.701836Z","iopub.execute_input":"2025-05-22T19:48:25.702106Z","iopub.status.idle":"2025-05-22T19:48:27.309868Z","shell.execute_reply.started":"2025-05-22T19:48:25.702076Z","shell.execute_reply":"2025-05-22T19:48:27.309316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"langs=[\"en\",\"tr\"]\npanx_ch = load_panx_datasets(langs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:48:27.310511Z","iopub.execute_input":"2025-05-22T19:48:27.310825Z","iopub.status.idle":"2025-05-22T19:48:41.311679Z","shell.execute_reply.started":"2025-05-22T19:48:27.310807Z","shell.execute_reply":"2025-05-22T19:48:41.311154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# number of data\nimport pandas as pd\n\npd.DataFrame(\n    {lang: [\n        panx_ch[lang][\"train\"].num_rows,\n        panx_ch[lang][\"validation\"].num_rows,\n        panx_ch[lang][\"test\"].num_rows\n    ] for lang in langs},\n    index=[\"Train\", \"Validation\", \"Test\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:48:41.313523Z","iopub.execute_input":"2025-05-22T19:48:41.313897Z","iopub.status.idle":"2025-05-22T19:48:41.335822Z","shell.execute_reply.started":"2025-05-22T19:48:41.313877Z","shell.execute_reply":"2025-05-22T19:48:41.335129Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2 EDA & Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Feature Items","metadata":{}},{"cell_type":"code","source":"#features\nfor key, value in panx_ch[\"en\"][\"train\"].features.items():\n    print(f\"{key}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:48:41.336570Z","iopub.execute_input":"2025-05-22T19:48:41.336823Z","iopub.status.idle":"2025-05-22T19:48:41.341912Z","shell.execute_reply.started":"2025-05-22T19:48:41.336790Z","shell.execute_reply":"2025-05-22T19:48:41.341016Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2 NER Tags","metadata":{}},{"cell_type":"code","source":"# NER Tags\ntags = panx_ch[\"en\"][\"train\"].features[\"ner_tags\"].feature\nprint(tags)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:48:41.342576Z","iopub.execute_input":"2025-05-22T19:48:41.342781Z","iopub.status.idle":"2025-05-22T19:48:41.353278Z","shell.execute_reply.started":"2025-05-22T19:48:41.342767Z","shell.execute_reply":"2025-05-22T19:48:41.352646Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3 An Example","metadata":{}},{"cell_type":"code","source":"# first context in English Train\nelement = panx_ch[\"en\"][\"train\"][2]\n\nfor key, value in element.items():\n    print(f\"{key}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:48:41.353819Z","iopub.execute_input":"2025-05-22T19:48:41.354001Z","iopub.status.idle":"2025-05-22T19:48:41.366132Z","shell.execute_reply.started":"2025-05-22T19:48:41.353987Z","shell.execute_reply":"2025-05-22T19:48:41.365405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ner_tags_int2str(batch):\n    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n\npanx_en = panx_ch[\"en\"].map(ner_tags_int2str)\n\nen_example = panx_en[\"train\"][2]\n\npd.DataFrame(\n    [en_example[\"tokens\"], en_example[\"ner_tags_str\"]],\n    index=[\"Tokens\", \"Tags\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:48:41.366945Z","iopub.execute_input":"2025-05-22T19:48:41.367259Z","iopub.status.idle":"2025-05-22T19:48:45.373456Z","shell.execute_reply.started":"2025-05-22T19:48:41.367241Z","shell.execute_reply":"2025-05-22T19:48:45.372679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# number of B- tags in en\n\nfrom collections import Counter\n\nsplit2freqs = defaultdict(Counter)\n\nfor split, dataset in panx_en.items():\n    for row in dataset[\"ner_tags_str\"]:\n        for tag in row:\n            if tag.startswith(\"B\"):\n                tag_type = tag.split(\"-\")[1]\n                split2freqs[split][tag_type] += 1\n\npd.DataFrame.from_dict(split2freqs, orient=\"index\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:48:45.374153Z","iopub.execute_input":"2025-05-22T19:48:45.374447Z","iopub.status.idle":"2025-05-22T19:48:45.865847Z","shell.execute_reply.started":"2025-05-22T19:48:45.374423Z","shell.execute_reply":"2025-05-22T19:48:45.865134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3 Tokenizer and Label Alignment","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nxlmr_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:48:45.868251Z","iopub.execute_input":"2025-05-22T19:48:45.868449Z","iopub.status.idle":"2025-05-22T19:49:00.678391Z","shell.execute_reply.started":"2025-05-22T19:48:45.868434Z","shell.execute_reply":"2025-05-22T19:49:00.677560Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.1 XLM-R Tokenization Mechanism (SentencePiece-based)","metadata":{}},{"cell_type":"code","source":"text_example = \"Mehmet Sahin lives in Halle!\"\n\nxlmr_tokens = xlmr_tokenizer(text_example).tokens()\n\npd.DataFrame([xlmr_tokens], index = [\"XLM-R\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:00.679339Z","iopub.execute_input":"2025-05-22T19:49:00.679866Z","iopub.status.idle":"2025-05-22T19:49:00.692588Z","shell.execute_reply.started":"2025-05-22T19:49:00.679827Z","shell.execute_reply":"2025-05-22T19:49:00.691855Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Token-level Entity Prediction","metadata":{}},{"cell_type":"code","source":"from transformers import XLMRobertaForTokenClassification\nimport torch\n\nxlmr_model_name = \"xlm-roberta-base\"\nindex2tag = {idx: tag for idx, tag in enumerate(tags.names)}\ntag2index = {tag: idx for idx, tag in enumerate(tags.names)}\nnum_labels = tags.num_classes\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nxlmr_model = XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name,\n                                                              num_labels = num_labels,\n                                                              id2label=index2tag,\n                                                              label2id=tag2index\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:00.693306Z","iopub.execute_input":"2025-05-22T19:49:00.693587Z","iopub.status.idle":"2025-05-22T19:49:19.288738Z","shell.execute_reply.started":"2025-05-22T19:49:00.693567Z","shell.execute_reply":"2025-05-22T19:49:19.288164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ner_infer(text, tags, model, tokenizer):\n    tokens = tokenizer(text).tokens()\n    input_ids = xlmr_tokenizer.encode(text,\n                                      return_tensors = \"pt\").to(device)\n    outputs = model(input_ids)[0]\n    predictions = torch.argmax(outputs, dim=2)\n    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n    return pd.DataFrame([tokens, preds], index = [\"Tokens\", \"Tags\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:19.289716Z","iopub.execute_input":"2025-05-22T19:49:19.290386Z","iopub.status.idle":"2025-05-22T19:49:19.295579Z","shell.execute_reply.started":"2025-05-22T19:49:19.290358Z","shell.execute_reply":"2025-05-22T19:49:19.294616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# a zero-shot example\nner_infer(text_example, tags, xlmr_model, xlmr_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:19.296400Z","iopub.execute_input":"2025-05-22T19:49:19.296667Z","iopub.status.idle":"2025-05-22T19:49:19.776838Z","shell.execute_reply.started":"2025-05-22T19:49:19.296645Z","shell.execute_reply":"2025-05-22T19:49:19.776288Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Label Alignment","metadata":{}},{"cell_type":"code","source":"def tokenize_and_align_labels(example_batch):\n    tokenized_batch = xlmr_tokenizer(\n        example_batch[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True\n    )\n\n    aligned_label_batch = []\n\n    for example_idx, word_labels in enumerate(example_batch[\"ner_tags\"]):\n        word_ids = tokenized_batch.word_ids(batch_index=example_idx)\n        previous_word_id = None\n        label_ids = []\n\n        for word_id in word_ids:\n            if word_id is None:\n                label_ids.append(-100)\n            elif word_id != previous_word_id:\n                label_ids.append(word_labels[word_id])\n            else:\n                label_ids.append(-100)\n            previous_word_id = word_id\n\n        aligned_label_batch.append(label_ids)\n\n    tokenized_batch[\"labels\"] = aligned_label_batch\n    return tokenized_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:19.777742Z","iopub.execute_input":"2025-05-22T19:49:19.778018Z","iopub.status.idle":"2025-05-22T19:49:19.784611Z","shell.execute_reply.started":"2025-05-22T19:49:19.777995Z","shell.execute_reply":"2025-05-22T19:49:19.783393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_panx_dataset(dataset_split):\n    return dataset_split.map(tokenize_and_align_labels,\n                     batched=True,\n                     remove_columns=[\"langs\", \"ner_tags\", \"tokens\"])\n\npanx_en_encoded = encode_panx_dataset(panx_ch[\"en\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:19.785834Z","iopub.execute_input":"2025-05-22T19:49:19.786489Z","iopub.status.idle":"2025-05-22T19:49:22.606377Z","shell.execute_reply.started":"2025-05-22T19:49:19.786465Z","shell.execute_reply":"2025-05-22T19:49:22.605586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example = panx_en_encoded[\"train\"][2]\n\ntokens = xlmr_tokenizer.convert_ids_to_tokens(example[\"input_ids\"])\nlabel_ids = example[\"labels\"]\nlabel_names = [index2tag[label] if label != -100 else \"IGN\" for label in label_ids]\n\npd.DataFrame(\n    [tokens, label_ids, label_names],\n    index=[\"Tokens\", \"Label IDs\", \"Labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:22.607523Z","iopub.execute_input":"2025-05-22T19:49:22.607731Z","iopub.status.idle":"2025-05-22T19:49:22.627660Z","shell.execute_reply.started":"2025-05-22T19:49:22.607715Z","shell.execute_reply":"2025-05-22T19:49:22.626686Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.4 Preparing Batched Inputs with Data Collator","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(xlmr_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:22.628596Z","iopub.execute_input":"2025-05-22T19:49:22.628895Z","iopub.status.idle":"2025-05-22T19:49:22.874457Z","shell.execute_reply.started":"2025-05-22T19:49:22.628874Z","shell.execute_reply":"2025-05-22T19:49:22.873660Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Model Training","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Defining Training Arguments","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\ndef get_training_arguments(output_dir=\"xlm-roberta-ner-multi\",\n                           num_epochs=3,\n                           batch_size=32,\n                           dataset_length=None,\n                           push_to_hf=False\n                          ):\n\n    return TrainingArguments(\n        output_dir=output_dir,\n        eval_strategy=\"epoch\",\n        save_strategy=\"no\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=num_epochs,\n        weight_decay=0.01,\n        logging_steps=dataset_length // batch_size,\n        push_to_hub=push_to_hf,\n        report_to=\"none\",\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:22.875295Z","iopub.execute_input":"2025-05-22T19:49:22.875474Z","iopub.status.idle":"2025-05-22T19:49:22.923546Z","shell.execute_reply.started":"2025-05-22T19:49:22.875459Z","shell.execute_reply":"2025-05-22T19:49:22.922567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.2 Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef align_predictions(predictions, label_ids):\n    pred_ids = np.argmax(predictions, axis=2)\n    batch_size, seq_len = pred_ids.shape\n\n    true_labels = []\n    pred_labels = []\n\n    for batch_idx in range(batch_size):\n        example_true = []\n        example_pred = []\n        for token_idx in range(seq_len):\n            true_label_id = label_ids[batch_idx][token_idx]\n            pred_label_id = pred_ids[batch_idx][token_idx]\n\n            if true_label_id != -100:\n                example_true.append(index2tag[true_label_id])\n                example_pred.append(index2tag[pred_label_id])\n\n        true_labels.append(example_true)\n        pred_labels.append(example_pred)\n\n    return pred_labels, true_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:22.924660Z","iopub.execute_input":"2025-05-22T19:49:22.924968Z","iopub.status.idle":"2025-05-22T19:49:22.929921Z","shell.execute_reply.started":"2025-05-22T19:49:22.924941Z","shell.execute_reply":"2025-05-22T19:49:22.929297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n\ndef compute_metrics(eval_pred):\n    predictions, label_ids = eval_pred\n    y_pred, y_true = align_predictions(predictions, label_ids)\n    return {\n        \"accuracy\": accuracy_score(y_true,y_pred),\n        \"precision\": precision_score(y_true, y_pred),\n        \"recall\": recall_score(y_true, y_pred),\n        \"f1\": f1_score(y_true, y_pred)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:22.930675Z","iopub.execute_input":"2025-05-22T19:49:22.931020Z","iopub.status.idle":"2025-05-22T19:49:22.944159Z","shell.execute_reply.started":"2025-05-22T19:49:22.930993Z","shell.execute_reply":"2025-05-22T19:49:22.943374Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.3 Training with Hugging Face Trainer","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntraining_args = get_training_arguments(\n    output_dir=\"xlm-roberta-ner-en\",\n    dataset_length=len(panx_en_encoded[\"train\"])\n)\n\nmodel_ft_en = Trainer(model = xlmr_model,\n                                   args = training_args,\n                                   tokenizer = xlmr_tokenizer,\n                                   train_dataset = panx_en_encoded[\"train\"],\n                                   eval_dataset = panx_en_encoded[\"validation\"],\n                                   data_collator = data_collator,\n                                   compute_metrics = compute_metrics\n                                  )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:22.944947Z","iopub.execute_input":"2025-05-22T19:49:22.945265Z","iopub.status.idle":"2025-05-22T19:49:24.498610Z","shell.execute_reply.started":"2025-05-22T19:49:22.945248Z","shell.execute_reply":"2025-05-22T19:49:24.498014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_ft_en.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T19:49:24.499409Z","iopub.execute_input":"2025-05-22T19:49:24.499659Z","iopub.status.idle":"2025-05-22T20:00:02.962474Z","shell.execute_reply.started":"2025-05-22T19:49:24.499637Z","shell.execute_reply":"2025-05-22T20:00:02.961729Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5 Cross-Lingual Training & Adaptation","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Phase 1: Fine-Tuned Model Evaluation on English (en)","metadata":{}},{"cell_type":"code","source":"text_en = \"Alan Mathison Turing was an English mathematician, computer scientist from London.\" \n\nner_infer(text_en, tags, model_ft_en.model, xlmr_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T20:00:02.963433Z","iopub.execute_input":"2025-05-22T20:00:02.964076Z","iopub.status.idle":"2025-05-22T20:00:02.990088Z","shell.execute_reply.started":"2025-05-22T20:00:02.964058Z","shell.execute_reply":"2025-05-22T20:00:02.989523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_f1_score(model, dataset):\n    return model.predict(dataset).metrics[\"test_f1\"]\n\nf1_scores_en = defaultdict(dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T20:00:02.990861Z","iopub.execute_input":"2025-05-22T20:00:02.991223Z","iopub.status.idle":"2025-05-22T20:00:02.995359Z","shell.execute_reply.started":"2025-05-22T20:00:02.991203Z","shell.execute_reply":"2025-05-22T20:00:02.994367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f1_scores_en[\"en\"] = get_f1_score(model_ft_en, panx_en_encoded[\"test\"])\nprint(f\"F1-score of [en] model on [en] dataset: {f1_scores_en['en']:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T20:00:02.996243Z","iopub.execute_input":"2025-05-22T20:00:02.996521Z","iopub.status.idle":"2025-05-22T20:00:33.937778Z","shell.execute_reply.started":"2025-05-22T20:00:02.996479Z","shell.execute_reply":"2025-05-22T20:00:33.937179Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.2 Phase 2: Zero-Shot Evaluation on Turkish","metadata":{}},{"cell_type":"code","source":"text_tr = \"Vardar Kapısı'ndan çıkarken nişanlarımı söktüm, biraz müteessirdim. Böyle yakındı Enver Paşa\"\n\nner_infer(text_tr, tags, model_ft_en.model, xlmr_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T20:00:33.938578Z","iopub.execute_input":"2025-05-22T20:00:33.938770Z","iopub.status.idle":"2025-05-22T20:00:33.968335Z","shell.execute_reply.started":"2025-05-22T20:00:33.938755Z","shell.execute_reply":"2025-05-22T20:00:33.967672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"panx_tr_encoded = encode_panx_dataset(panx_ch[\"tr\"])\nf1_scores_en[\"tr\"] = get_f1_score(model_ft_en, panx_tr_encoded[\"test\"])\nprint(f\"F1-score of [en] model on [tr] dataset: {f1_scores_en['tr']:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T20:00:33.971365Z","iopub.execute_input":"2025-05-22T20:00:33.971727Z","iopub.status.idle":"2025-05-22T20:01:06.611910Z","shell.execute_reply.started":"2025-05-22T20:00:33.971710Z","shell.execute_reply":"2025-05-22T20:01:06.611161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.3 Phase 3: Progressive Adaptation to Turkish (Few-Shot Fine-Tuning)","metadata":{}},{"cell_type":"code","source":"# subset training function for tr\ndef train_on_subset(dataset,\n                    num_samples,\n                    output_dir=\"ner-subset\"\n                   ):\n\n    train_subset = dataset[\"train\"].shuffle(seed=42).select(range(num_samples))\n    val_set = dataset[\"validation\"]\n    test_ds = dataset[\"test\"]\n    \n    training_args = get_training_arguments(output_dir,\n                                           dataset_length=len(train_subset),\n                                           push_to_hf=False\n                                          )\n\n    sub_model_trainer = Trainer(model = model_ft_en.model,\n                           args = training_args,\n                           tokenizer = xlmr_tokenizer,\n                           train_dataset = train_subset,\n                           eval_dataset = val_set,\n                           data_collator = data_collator,\n                           compute_metrics = compute_metrics\n                                      )\n\n    sub_model_trainer.train()\n    # sub_model_trainer.push_to_hub(commit_message=\"Training has been completed successfully!\")\n\n    f1_score = get_f1_score(sub_model_trainer, test_ds)\n    return pd.DataFrame.from_dict(\n                {\"num_samples\": [len(train_subset)], \"f1_score\": [f1_score]})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T20:01:06.612769Z","iopub.execute_input":"2025-05-22T20:01:06.613019Z","iopub.status.idle":"2025-05-22T20:01:06.618427Z","shell.execute_reply.started":"2025-05-22T20:01:06.613002Z","shell.execute_reply":"2025-05-22T20:01:06.617735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics_df = pd.DataFrame()\n\nfor size in [250, 500, 1000, 2000, 5000, 10000, 20000]:\n    metrics_df = metrics_df._append(train_on_subset(panx_tr_encoded, num_samples=size, output_dir=\"xlm-roberta-base-cased-ner-turkish\"),\n                                   ignore_index = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T20:01:06.619263Z","iopub.execute_input":"2025-05-22T20:01:06.619528Z","iopub.status.idle":"2025-05-22T20:31:10.236926Z","shell.execute_reply.started":"2025-05-22T20:01:06.619507Z","shell.execute_reply":"2025-05-22T20:31:10.235927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test scores\nmetrics_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T20:31:10.237869Z","iopub.execute_input":"2025-05-22T20:31:10.238420Z","iopub.status.idle":"2025-05-22T20:31:10.247403Z","shell.execute_reply.started":"2025-05-22T20:31:10.238394Z","shell.execute_reply":"2025-05-22T20:31:10.246576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.axhline(f1_scores_en['tr'], ls = \"--\", color=\"r\")\nmetrics_df.set_index(\"num_samples\").plot(ax=ax)\nplt.legend([\"Phase 2: Zero-Shot Evaluation on Turkish\", \"Phase 3: Few-Shot Evaluation on Turkish\"], loc=\"lower right\")\nplt.ylim((0,1))\nplt.xlabel(\"Number of Training Turkish Samples\")\nplt.ylabel(\"F1 Score\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T20:31:10.248378Z","iopub.execute_input":"2025-05-22T20:31:10.248648Z","iopub.status.idle":"2025-05-22T20:31:10.522418Z","shell.execute_reply.started":"2025-05-22T20:31:10.248624Z","shell.execute_reply":"2025-05-22T20:31:10.521604Z"}},"outputs":[],"execution_count":null}]}